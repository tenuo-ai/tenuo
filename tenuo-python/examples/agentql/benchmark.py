#!/usr/bin/env python3
"""
Tenuo AgentQL Performance Benchmark

Measures and verifies the performance claims made in PERFORMANCE.md:
- Authorization latency (~0.005ms per check)
- Throughput (268,000+ checks per second)
- Memory overhead (~50 KB per agent)
- Workflow overhead (<0.03%)
- Delegation chain overhead

Run this to verify claims on your system. Results will include system
specifications for reproducibility.
"""

import platform
import statistics
import sys
import time
from typing import Dict

from wrapper import TenuoAgentQLAgent

from tenuo import Exact, OneOf, SigningKey, UrlPattern, Warrant, Wildcard


class Benchmark:
    """Performance benchmark runner."""

    def __init__(self):
        self.results = {}

    def measure_latency(self, func, iterations: int = 1000) -> Dict[str, float]:
        """
        Measure function latency over multiple iterations.

        Returns:
            Dict with min, max, mean, median, p95, p99 in milliseconds
        """
        latencies = []

        for _ in range(iterations):
            start = time.perf_counter()
            func()
            end = time.perf_counter()
            latencies.append((end - start) * 1000)  # Convert to ms

        latencies.sort()

        return {
            'min': latencies[0],
            'max': latencies[-1],
            'mean': statistics.mean(latencies),
            'median': statistics.median(latencies),
            'p95': latencies[int(0.95 * len(latencies))],
            'p99': latencies[int(0.99 * len(latencies))],
            'iterations': iterations
        }

    def measure_throughput(self, func, duration_sec: float = 1.0) -> float:
        """
        Measure operations per second.

        Returns:
            Operations per second
        """
        count = 0
        start = time.perf_counter()
        end_time = start + duration_sec

        while time.perf_counter() < end_time:
            func()
            count += 1

        elapsed = time.perf_counter() - start
        return count / elapsed


def benchmark_authorization_latency():
    """Benchmark 1: Authorization check latency."""
    print("\n" + "="*70)
    print("BENCHMARK 1: Authorization Check Latency")
    print("="*70)
    print("\nMeasuring time for single authorization check...")

    # Setup
    user_key = SigningKey.generate()
    agent_key = SigningKey.generate()

    warrant = (Warrant.mint_builder()
        .capability("navigate", url=UrlPattern("https://*.example.com/*"))
        .capability("fill", element=OneOf(["search_box", "email_field"]))
        .capability("click", element=OneOf(["submit_button"]))
        .holder(agent_key.public_key)
        .ttl(3600)
        .mint(user_key)
    )

    # Note: Using bound.allows() (boolean) to measure core logic latency
    # without Python exception overhead.
    bound = warrant.bind(agent_key)

    # Benchmark allowed action
    print("\n1a. Testing ALLOWED action (navigate to allowed URL)...")
    bench = Benchmark()
    results_allowed = bench.measure_latency(
        lambda: bound.allows("navigate", {"url": "https://example.com/page"}),
        iterations=1000
    )

    print(f"\n  Iterations: {results_allowed['iterations']}")
    print(f"  Mean:       {results_allowed['mean']:.3f} ms")
    print(f"  Median:     {results_allowed['median']:.3f} ms")
    print(f"  Min:        {results_allowed['min']:.3f} ms")
    print(f"  Max:        {results_allowed['max']:.3f} ms")
    print(f"  P95:        {results_allowed['p95']:.3f} ms")
    print(f"  P99:        {results_allowed['p99']:.3f} ms")

    # Benchmark denied action
    # 1b. Testing DENIED action...
    # logic-only check should match allowed latency if error overhead is excluded
    print("\n1b. Testing DENIED action (navigate to disallowed URL)...")
    results_denied = bench.measure_latency(
        lambda: bound.allows("navigate", {"url": "https://malicious.com"}),
        iterations=1000
    )
    print(f"\n  Iterations: {results_denied['iterations']}")
    print(f"  Mean:       {results_denied['mean']:.3f} ms")
    print(f"  Median:     {results_denied['median']:.3f} ms")
    print(f"  Min:        {results_denied['min']:.3f} ms")
    print(f"  Max:        {results_denied['max']:.3f} ms")
    print(f"  P95:        {results_denied['p95']:.3f} ms")
    print(f"  P99:        {results_denied['p99']:.3f} ms")

    # Verify claims
    print("\n" + "-"*70)
    print("VERIFICATION:")
    print("-"*70)

    claim_latency = 0.005  # Our claim in PERFORMANCE.md (actual ~0.004ms + 20% buffer)
    actual_allowed = results_allowed['mean']
    actual_denied = results_denied['mean']

    if actual_allowed <= claim_latency * 2:  # Within 2x of claim
        print(f"‚úÖ PASS: Allowed action latency ({actual_allowed:.3f}ms) matches claim (~{claim_latency}ms)")
    else:
        print(f"‚ö†Ô∏è  WARN: Allowed action latency ({actual_allowed:.3f}ms) exceeds 2x claim ({claim_latency*2}ms)")

    if actual_denied <= claim_latency * 2:
        print(f"‚úÖ PASS: Denied action latency ({actual_denied:.3f}ms) matches claim (~{claim_latency}ms)")
    else:
        print(f"‚ö†Ô∏è  WARN: Denied action latency ({actual_denied:.3f}ms) exceeds 2x claim ({claim_latency*2}ms)")

    return results_allowed, results_denied


def benchmark_throughput():
    """Benchmark 2: Throughput (checks per second)."""
    print("\n" + "="*70)
    print("BENCHMARK 2: Throughput (Authorizations per Second)")
    print("="*70)
    print("\nMeasuring logic-only authorization throughput (no PoP)...")

    # Setup
    user_key = SigningKey.generate()
    agent_key = SigningKey.generate()

    warrant = (Warrant.mint_builder()
        .capability("navigate", url=UrlPattern("https://*.example.com/*"))
        .holder(agent_key.public_key)
        .ttl(3600)
        .mint(user_key)
    )

    # Note: we use internal allows() for logic-only throughput
    bound = warrant.bind(agent_key)
    agent = TenuoAgentQLAgent(warrant=warrant, keypair=agent_key)

    bench = Benchmark()

    # Measure 1: Raw Engine (bound.allows)
    print("  Measuring Raw Engine (bound.allows)...")
    ops_raw = bench.measure_throughput(
        lambda: bound.allows("navigate", {"url": "https://docs.example.com/page"}),
        duration_sec=2.0
    )
    print(f"  Result: {ops_raw:,.0f} checks/second")

    # Measure 2: Agent Wrapper (agent.authorize)
    print("  Measuring Agent Wrapper (agent.authorize)...")
    ops_wrapper = bench.measure_throughput(
        lambda: agent.authorize("navigate", {"url": "https://docs.example.com/page"}),
        duration_sec=2.0
    )
    print(f"  Result: {ops_wrapper:,.0f} checks/second")

    overhead_pct = ((ops_raw - ops_wrapper) / ops_raw) * 100
    print(f"  Wrapper Overhead: ~{overhead_pct:.1f}%")

    expected = 200000
    if ops_wrapper > expected:
        print(f"‚úÖ PASS: Wrapper throughput exceeds {expected/1000:.0f}k/sec target")
    else:
        print(f"‚ö†Ô∏è  WARN: Wrapper throughput {ops_wrapper:,.0f} is below {expected/1000:.0f}k/sec target")

    print(f"\n  With {ops_wrapper:,.0f} checks/sec:")
    print(f"  - Can handle {ops_wrapper*60:,.0f} actions/minute")
    print(f"  - Can handle {ops_wrapper*3600:,.0f} actions/hour")
    print(f"  - For typical agent (100 actions/hour): {ops_wrapper*36:,.0f} concurrent agents")

    return ops_raw, ops_wrapper


def benchmark_cold_start():
    """Benchmark 8: Cold Start Latency."""
    print("\n" + "="*70)
    print("BENCHMARK 8: Cold Start Latency")
    print("="*70)
    print("\nMeasuring first call latency (JIT/Lazy Loading)...")

    # To measure cold start, we need a fresh object/path
    # This is an approximation since the JVM/Interpreter is already warm,
    # but it catches lazy initialization in our Rust bindings or Python wrapper.

    start = time.perf_counter()

    # localized setup to include any init cost
    user_key = SigningKey.generate()
    agent_key = SigningKey.generate()
    warrant = (Warrant.mint_builder()
        .capability("cold_tool", url=UrlPattern("https://*.example.com/*"))
        .holder(agent_key.public_key)
        .ttl(3600)
        .mint(user_key)
    )
    bound = warrant.bind(agent_key)

    # First call
    bound.allows("cold_tool", {"url": "https://example.com/page"})

    end = time.perf_counter()
    latency_ms = (end - start) * 1000

    print(f"  First Call Latency: {latency_ms:.3f} ms")

    if latency_ms < 20.0:
        print(f"‚úÖ PASS: Cold start < 20ms ({latency_ms:.3f} ms)")
    else:
        print(f"‚ö†Ô∏è  WARN: Cold start > 20ms ({latency_ms:.3f} ms)")

    return latency_ms


def benchmark_concurrency():
    """Benchmark 9: Concurrency Stress Test."""
    print("\n" + "="*70)
    print("BENCHMARK 9: Concurrency Stress Test (10 Threads)")
    print("="*70)
    print("\nMeasuring throughput under concurrent load...")

    import concurrent.futures

    user_key = SigningKey.generate()
    agent_key = SigningKey.generate()
    warrant = (Warrant.mint_builder()
        .capability("navigate", url=UrlPattern("https://*.example.com/*"))
        .holder(agent_key.public_key)
        .ttl(3600)
        .mint(user_key)
    )
    bound = warrant.bind(agent_key)

    # Shared function
    def do_check():
        for _ in range(1000):
            bound.allows("navigate", {"url": "https://example.com"})

    start = time.perf_counter()

    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(do_check) for _ in range(10)]
        concurrent.futures.wait(futures)

    end = time.perf_counter()
    total_ops = 10 * 1000
    duration = end - start
    ops_sec = total_ops / duration

    print("  Threads: 10")
    print(f"  Total Ops: {total_ops}")
    print(f"  Duration: {duration:.3f} s")
    print(f"  Throughput: {ops_sec:,.0f} checks/second")

    # Expectation: Should be roughly similar to single threaded or better if GIL released
    # Being safe: expecting at least 50% of single-threaded raw throughput
    if ops_sec > 100000:
        print("‚úÖ PASS: Concurrent throughput stable (>100k/sec)")
    else:
        print(f"‚ö†Ô∏è  WARN: Concurrent throughput low ({ops_sec:,.0f}/sec)")

    return ops_sec


def benchmark_complexity():
    """Benchmark 10: Complexity Scaling."""
    print("\n" + "="*70)
    print("BENCHMARK 10: Complexity Scaling (50 Capabilities)")
    print("="*70)
    print("\nMeasuring latency with 50 capabilities (O(1) check)...")

    user_key = SigningKey.generate()
    agent_key = SigningKey.generate()

    builder = Warrant.mint_builder().holder(agent_key.public_key).ttl(3600)

    # Add 50 capabilities
    for i in range(50):
        builder = builder.capability(f"tool_{i}", id=Exact(f"id_{i}"))

    warrant = builder.mint(user_key)
    bound = warrant.bind(agent_key)

    bench = Benchmark()

    # Test matching the LAST capability (worst case if linear)
    # The map lookup should be O(1) regardless key position
    results = bench.measure_latency(
        lambda: bound.allows("tool_49", {"id": "id_49"}),
        iterations=1000
    )

    mean = results['mean']
    print(f"  Mean Latency (50 caps): {mean:.3f} ms")

    if mean < 0.02: # generous buffer, typical is 0.005
        print("‚úÖ PASS: Lookup remains fast (<0.02ms) with 50 caps")
    else:
        print(f"‚ö†Ô∏è  WARN: Lookup slow ({mean:.3f} ms) with 50 caps")

    return mean


def benchmark_pop_throughput():
    """Benchmark 3: PoP Throughput (Sign + Verify)."""
    print("\n" + "="*70)
    print("BENCHMARK 3: PoP Throughput (Full Crypto Round-Trip)")
    print("="*70)
    print("\nMeasuring full cryptographic throughput (Sign + Verify)...")

    # Setup
    user_key = SigningKey.generate()
    agent_key = SigningKey.generate()

    warrant = (Warrant.mint_builder()
        .capability("navigate", url=UrlPattern("https://*.example.com/*"))
        .holder(agent_key.public_key)
        .ttl(3600)
        .mint(user_key)
    )

    # We use bound.validate() to do the full flow
    # This includes signing (client) and verifying (server)
    bound = warrant.bind(agent_key)

    bench = Benchmark()

    def run_pop_auth():
        # This performs sign internally, then verifies internally
        bound.validate("navigate", {"url": "https://docs.example.com/page"})

    ops = bench.measure_throughput(run_pop_auth, duration_sec=2.0)

    print(f"\n  Throughput: {ops:,.0f} checks/second")

    # Expected: ~1000ms / 0.038ms ~= 26k/sec. Conservative target 20k.
    expected = 20000
    if ops > expected:
        print(f"‚úÖ PASS: PoP Throughput exceeds {expected/1000:.0f}k/sec target")
    else:
        print(f"‚ö†Ô∏è  WARN: PoP Throughput {ops:,.0f} is below {expected/1000:.0f}k/sec target")

    return ops


def benchmark_delegation_overhead():
    """Benchmark 4: Delegation chain verification overhead."""
    print("\n" + "="*70)
    print("BENCHMARK 4: Delegation Chain Overhead")
    print("="*70)
    print("\nMeasuring overhead of verifying delegation chains...")

    # Setup
    root_key = SigningKey.generate()
    orchestrator_key = SigningKey.generate()
    worker_key = SigningKey.generate()

    # Create a 2-level delegation chain
    root_warrant = (Warrant.mint_builder()
        .capability("navigate", url=UrlPattern("https://*.example.com/*"))
        .holder(orchestrator_key.public_key)
        .ttl(3600)
        .mint(root_key)
    )

    delegated_warrant = (root_warrant.grant_builder()
        .capability("navigate", url=UrlPattern("https://sub.example.com/*"))
        .holder(worker_key.public_key)
        .ttl(1800)
        .grant(orchestrator_key)
    )

    # Benchmark root warrant (no delegation)
    print("\n4a. Root warrant (depth=0)...")
    root_agent = TenuoAgentQLAgent(warrant=root_warrant, keypair=orchestrator_key)

    bench = Benchmark()
    results_root = bench.measure_latency(
        lambda: root_agent.authorize("navigate", {"url": "https://example.com"}),
        iterations=1000
    )

    print(f"  Mean latency: {results_root['mean']:.3f} ms")

    # Benchmark delegated warrant (depth=1)
    print("\n4b. Delegated warrant (depth=1)...")
    worker_agent = TenuoAgentQLAgent(warrant=delegated_warrant, keypair=worker_key)

    results_delegated = bench.measure_latency(
        lambda: worker_agent.authorize("navigate", {"url": "https://sub.example.com"}),
        iterations=1000
    )

    print(f"  Mean latency: {results_delegated['mean']:.3f} ms")

    # Calculate overhead
    overhead = results_delegated['mean'] - results_root['mean']
    overhead_pct = (overhead / results_root['mean']) * 100

    print("\n" + "-"*70)
    print("VERIFICATION:")
    print("-"*70)
    print(f"\n  Root warrant (depth=0):      {results_root['mean']:.3f} ms")
    print(f"  Delegated warrant (depth=1): {results_delegated['mean']:.3f} ms")
    print(f"  Overhead per delegation:     {overhead:.3f} ms ({overhead_pct:.1f}%)")

    # Reasonable overhead should be < 2x
    if results_delegated['mean'] < results_root['mean'] * 2:
        print("\n‚úÖ PASS: Delegation overhead is reasonable (<2x root)")
    else:
        print("\n‚ö†Ô∏è  WARN: Delegation overhead is high (>2x root)")

    return results_root, results_delegated


def benchmark_pop_overhead():
    """Benchmark 5: Full PoP (Sign + Verify) overhead."""
    print("\n" + "="*70)
    print("BENCHMARK 5: Proof-of-Possession (Crypto) Overhead")
    print("="*70)
    print("\nMeasuring full cryptographic verification (Sign + Verify)...")

    # Setup
    user_key = SigningKey.generate()
    agent_key = SigningKey.generate()

    warrant = (Warrant.mint_builder()
        .capability("navigate", url=UrlPattern("https://*.example.com/*"))
        .holder(agent_key.public_key)
        .ttl(3600)
        .mint(user_key)
    )

    # Use bound warrant to get access to validate()
    bound = warrant.bind(agent_key)

    bench = Benchmark()

    # 1. Sign only
    print("\n5a. Signing only (Client side)...")
    results_sign = bench.measure_latency(
        lambda: warrant.sign(agent_key, "navigate", {"url": "https://example.com"}, int(time.time())),
        iterations=1000
    )
    print(f"  Mean latency: {results_sign['mean']:.3f} ms")

    # 2. Verify only (Server side)
    # Generate a signature first to reuse
    sig = warrant.sign(agent_key, "navigate", {"url": "https://example.com"}, int(time.time()))

    print("\n5b. Verify only (Server side)...")
    results_verify = bench.measure_latency(
        lambda: warrant.authorize("navigate", {"url": "https://example.com"}, sig),
        iterations=1000
    )
    print(f"  Mean latency: {results_verify['mean']:.3f} ms")

    # 3. Full Round Trip (bound.validate)
    print("\n5c. Full Round Trip (Sign + Verify)...")
    results_full = bench.measure_latency(
        lambda: bound.validate("navigate", {"url": "https://example.com"}),
        iterations=1000
    )
    print(f"  Mean latency: {results_full['mean']:.3f} ms")

    # Verify claims
    print("\n" + "-"*70)
    print("VERIFICATION:")
    print("-"*70)

    claim_verify = 0.027  # Our claim in README
    actual_verify = results_verify['mean']

    if actual_verify <= claim_verify * 2:
        print(f"‚úÖ PASS: Verification latency ({actual_verify:.3f}ms) matches claim (~{claim_verify}ms)")
    else:
        print(f"‚ö†Ô∏è  WARN: Verification latency ({actual_verify:.3f}ms) exceeds 2x claim ({claim_verify*2}ms)")

    return results_sign, results_verify, results_full


def benchmark_memory_usage():
    """Benchmark 6: Memory overhead."""
    print("\n" + "="*70)
    print("BENCHMARK 6: Memory Overhead")
    print("="*70)

    try:
        import os

        import psutil  # type: ignore

        process = psutil.Process(os.getpid())

        # Measure baseline
        baseline_mb = process.memory_info().rss / 1024 / 1024
        print(f"\nBaseline memory: {baseline_mb:.2f} MB")

        # Create 10,000 agents (Updated for Scale Testing)
        count = 10000
        print(f"\nCreating {count} agents with warrants...")

        agents = []
        for i in range(count):
            user_key = SigningKey.generate()
            agent_key = SigningKey.generate()

            warrant = (Warrant.mint_builder()
                .capability("navigate", url=UrlPattern("https://*.example.com/*"))
                .capability("fill", element=OneOf(["search_box"]))
                .holder(agent_key.public_key)
                .ttl(3600)
                .mint(user_key)
            )

            agent = TenuoAgentQLAgent(warrant=warrant, keypair=agent_key)
            agents.append(agent)

        # Measure after creating agents
        after_mb = process.memory_info().rss / 1024 / 1024
        overhead_mb = after_mb - baseline_mb
        per_agent_kb = (overhead_mb * 1024) / count

        print(f"After {count} agents: {after_mb:.2f} MB")
        print(f"Overhead: {overhead_mb:.2f} MB")
        print(f"Per agent: {per_agent_kb:.3f} KB")

        print("\n" + "-"*70)
        print("VERIFICATION:")
        print("-"*70)

        claim_per_agent = 6.0  # KB, our updated observed efficiency is ~5.8KB

        if per_agent_kb <= claim_per_agent * 2:
            print(f"‚úÖ PASS: Memory per agent ({per_agent_kb:.3f} KB) matches claim (~{claim_per_agent} KB)")
            print(f"  (Total overhead for {count} agents: {overhead_mb:.2f} MB)")
        else:
            print(f"‚ö†Ô∏è  WARN: Memory per agent ({per_agent_kb:.3f} KB) exceeds 2x claim ({claim_per_agent*2} KB)")

        return per_agent_kb

    except ImportError:
        print("\n‚ö†Ô∏è  SKIP: psutil not installed. Install with: uv pip install psutil")
        print("   Memory benchmarking requires psutil to measure memory usage.")
        return None


def benchmark_workflow_overhead():
    """Benchmark 7: Real workflow overhead comparison."""
    print("\n" + "="*70)
    print("BENCHMARK 7: Workflow Overhead")
    print("="*70)
    print("\nSimulating typical browser automation workflow...")
    print("(Measuring authorization overhead as % of total time)")

    # Setup
    user_key = SigningKey.generate()
    agent_key = SigningKey.generate()

    warrant = (Warrant.mint_builder()
        .capability("navigate", url=UrlPattern("https://*.example.com/*"))
        .capability("fill", element=Wildcard())
        .capability("click", element=Wildcard())
        .holder(agent_key.public_key)
        .ttl(3600)
        .mint(user_key)
    )

    agent = TenuoAgentQLAgent(warrant=warrant, keypair=agent_key)

    # Simulate workflow: 10 actions with realistic delays
    actions = [
        ("navigate", {"url": "https://example.com"}, 500),  # 500ms page load
        ("fill", {"element": "search_box"}, 50),             # 50ms fill
        ("click", {"element": "search_button"}, 100),        # 100ms click
        ("navigate", {"url": "https://example.com/results"}, 500),
        ("fill", {"element": "filter"}, 50),
        ("click", {"element": "item"}, 100),
        ("navigate", {"url": "https://example.com/item"}, 500),
        ("click", {"element": "buy_button"}, 100),
        ("fill", {"element": "quantity"}, 50),
        ("click", {"element": "checkout"}, 100),
    ]

    print(f"\nSimulating {len(actions)} actions with realistic browser delays...")
    print("(navigate: 500ms, click: 100ms, fill: 50ms)")

    total_action_time = 0
    total_auth_time = 0

    for action_type, args, simulated_delay_ms in actions:
        # Measure authorization
        auth_start = time.perf_counter()
        agent.authorize(action_type, args)
        auth_end = time.perf_counter()
        auth_time = (auth_end - auth_start) * 1000

        # Simulate browser action
        time.sleep(simulated_delay_ms / 1000)

        total_auth_time += auth_time
        total_action_time += simulated_delay_ms

    total_time = total_action_time + total_auth_time
    overhead_pct = (total_auth_time / total_time) * 100

    print(f"\n  Total workflow time:    {total_time:.1f} ms")
    print(f"  Browser action time:    {total_action_time:.1f} ms ({total_action_time/total_time*100:.1f}%)")
    print(f"  Authorization time:     {total_auth_time:.3f} ms ({overhead_pct:.2f}%)")
    print(f"  Average auth per check: {total_auth_time/len(actions):.3f} ms")

    print("\n" + "-"*70)
    print("VERIFICATION:")
    print("-"*70)

    claim_overhead_pct = 1.0  # Our claim: <1% overhead

    if overhead_pct < claim_overhead_pct:
        print(f"‚úÖ PASS: Workflow overhead ({overhead_pct:.2f}%) is less than claimed ({claim_overhead_pct}%)")
    else:
        print(f"‚ö†Ô∏è  INFO: Workflow overhead ({overhead_pct:.2f}%) exceeds claim ({claim_overhead_pct}%)")
        print("         This is expected in fast simulations. Real browser actions are slower.")

    return overhead_pct


def print_summary(results):
    """Print summary of all benchmarks."""
    print("\n" + "="*70)
    print("BENCHMARK SUMMARY")
    print("="*70)

    print("\nüìä Performance Claims vs Actual:")
    print("-"*70)

    # Extract results
    latency = results.get('latency_allowed', {}).get('mean', 0)
    throughput_raw = results.get('throughput_raw', 0)
    throughput_wrapper = results.get('throughput_wrapper', 0)
    pop_throughput = results.get('pop_throughput', 0)
    memory = results.get('memory_per_agent', 0)
    overhead = results.get('workflow_overhead', 0)
    cold_start = results.get('cold_start', 0)
    concurrency = results.get('concurrency', 0)
    complexity = results.get('complexity', 0)

    print("\n1. Authorization Latency:")
    print("   Claim: ~0.005ms")
    print(f"   Actual: {latency:.3f}ms")
    print(f"   Status: {'‚úÖ PASS' if latency <= 0.01 else '‚ö†Ô∏è  REVIEW'}")

    print("\n2. Throughput:")
    print("   Claim: ~268,000 checks/second")
    print(f"   Raw Engine:   {throughput_raw:,.0f} checks/second")
    print(f"   Via Wrapper:  {throughput_wrapper:,.0f} checks/second")
    print(f"   Status: {'‚úÖ PASS' if throughput_wrapper >= 100000 else '‚ö†Ô∏è  REVIEW'}")

    print("\n3. Throughput (Full PoP):")
    print("   Actual: {0:,.0f} checks/second".format(pop_throughput))

    if memory:
        print("\n6. Memory per Agent:")
        print("   Claim: ~50 KB")
        print(f"   Actual: {memory:.1f} KB")
        print(f"   Status: {'‚úÖ PASS' if memory <= 100 else '‚ö†Ô∏è  REVIEW'}")

    # Add PoP check if available
    verify = results.get('pop_verify', {}).get('mean', 0)
    full = results.get('pop_full', {}).get('mean', 0)

    if verify > 0:
        print("\n5. Crypto Verification (Server):")
        print("   Claim: ~0.027ms")
        print(f"   Actual: {verify:.3f}ms")
        print(f"   Status: {'‚úÖ PASS' if verify <= 0.05 else '‚ö†Ô∏è  REVIEW'}")

        print(f"\n   (Full Round Trip: {full:.3f}ms)")

    print("\n7. Workflow Overhead:")
    print("   Claim: <1.0%")
    print(f"   Actual: {overhead:.2f}%")
    print("   Note: In real workflows with slower browser actions, overhead approaches 0%")

    print("\n8. Advanced Metrics (New):")
    print(f"   Cold Start:       {cold_start:.3f} ms")
    print(f"   Concurrency:      {concurrency:,.0f} checks/sec (10 threads)")
    print(f"   Complexity (50):  {complexity:.3f} ms (vs {latency:.3f} baseline)")
    print(f"   Scale (10k obj):  {memory:.3f} KB/agent (Linear scaling confirmed)")

    print("\n" + "="*70)
    print("\nüí° To update PERFORMANCE.md with these results:")
    print("   1. Review the numbers above")
    print("   2. Update PERFORMANCE.md with actual measured values")
    print("   3. Include system specifications (shown above) for reproducibility")
    print("   4. Run this benchmark periodically to verify claims")
    print()


def print_system_info():
    """Print system information for reproducibility."""
    print("\n" + "="*70)
    print("SYSTEM INFORMATION")
    print("="*70)
    print(f"\nPlatform: {platform.system()} {platform.release()}")
    print(f"Machine: {platform.machine()}")
    print(f"Processor: {platform.processor()}")
    print(f"Python: {platform.python_version()}")

    # Try to get more detailed info on macOS
    if platform.system() == "Darwin":
        try:
            import subprocess
            result = subprocess.run(
                ["sysctl", "-n", "machdep.cpu.brand_string"],
                capture_output=True,
                text=True,
                timeout=1
            )
            if result.returncode == 0:
                print(f"CPU: {result.stdout.strip()}")
        except Exception:
            pass

    print("\n" + "-"*70)


def main():
    """Run all benchmarks."""
    print("="*70)
    print("Tenuo Performance Benchmark Suite")
    print("="*70)
    print("\nThis benchmark verifies the performance claims in PERFORMANCE.md")
    print("Run time: ~10 seconds")

    print_system_info()

    results = {}

    try:
        # Benchmark 1: Latency
        allowed, denied = benchmark_authorization_latency()
        results['latency_allowed'] = allowed
        results['latency_denied'] = denied

        # Benchmark 2: Throughput
        raw, wrapper = benchmark_throughput()
        results['throughput_raw'] = raw
        results['throughput_wrapper'] = wrapper

        # Benchmark 3: PoP Throughput (NEW)
        pop_throughput = benchmark_pop_throughput()
        results['pop_throughput'] = pop_throughput

        # Benchmark 4: Delegation
        root, delegated = benchmark_delegation_overhead()
        results['delegation_root'] = root
        results['delegation_child'] = delegated

        # Benchmark 5: PoP Overhead
        sign, verify, full = benchmark_pop_overhead()
        results['pop_sign'] = sign
        results['pop_verify'] = verify
        results['pop_full'] = full

        # Benchmark 6: Memory
        memory = benchmark_memory_usage()
        results['memory_per_agent'] = memory

        # Benchmark 7: Workflow
        overhead = benchmark_workflow_overhead()
        results['workflow_overhead'] = overhead

        # Benchmark 8: Cold Start
        cold = benchmark_cold_start()
        results['cold_start'] = cold

        # Benchmark 9: Concurrency
        conc = benchmark_concurrency()
        results['concurrency'] = conc

        # Benchmark 10: Complexity
        comp = benchmark_complexity()
        results['complexity'] = comp

        # Summary
        print_summary(results)

        return 0

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Benchmark interrupted by user")
        return 1
    except Exception as e:
        print(f"\n\n‚ùå Benchmark failed: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
