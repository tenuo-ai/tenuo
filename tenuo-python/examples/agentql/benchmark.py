#!/usr/bin/env python3
"""
Tenuo AgentQL Performance Benchmark

Measures and verifies the performance claims made in PERFORMANCE.md:
- Authorization latency (~0.005ms per check)
- Throughput (268,000+ checks per second)
- Memory overhead (~50 KB per agent)
- Workflow overhead (<0.03%)
- Delegation chain overhead

Run this to verify claims on your system. Results will include system
specifications for reproducibility.
"""

import time
import sys
import statistics
import platform
from typing import Dict

from tenuo import Warrant, SigningKey, OneOf, UrlPattern, Wildcard
from wrapper import TenuoAgentQLAgent


class Benchmark:
    """Performance benchmark runner."""

    def __init__(self):
        self.results = {}

    def measure_latency(self, func, iterations: int = 1000) -> Dict[str, float]:
        """
        Measure function latency over multiple iterations.

        Returns:
            Dict with min, max, mean, median, p95, p99 in milliseconds
        """
        latencies = []

        for _ in range(iterations):
            start = time.perf_counter()
            func()
            end = time.perf_counter()
            latencies.append((end - start) * 1000)  # Convert to ms

        latencies.sort()

        return {
            'min': latencies[0],
            'max': latencies[-1],
            'mean': statistics.mean(latencies),
            'median': statistics.median(latencies),
            'p95': latencies[int(0.95 * len(latencies))],
            'p99': latencies[int(0.99 * len(latencies))],
            'iterations': iterations
        }

    def measure_throughput(self, func, duration_sec: float = 1.0) -> float:
        """
        Measure operations per second.

        Returns:
            Operations per second
        """
        count = 0
        start = time.perf_counter()
        end_time = start + duration_sec

        while time.perf_counter() < end_time:
            func()
            count += 1

        elapsed = time.perf_counter() - start
        return count / elapsed


def benchmark_authorization_latency():
    """Benchmark 1: Authorization check latency."""
    print("\n" + "="*70)
    print("BENCHMARK 1: Authorization Check Latency")
    print("="*70)
    print("\nMeasuring time for single authorization check...")

    # Setup
    user_key = SigningKey.generate()
    agent_key = SigningKey.generate()

    warrant = (Warrant.mint_builder()
        .capability("navigate", url=UrlPattern("https://*.example.com/*"))
        .capability("fill", element=OneOf(["search_box", "email_field"]))
        .capability("click", element=OneOf(["submit_button"]))
        .holder(agent_key.public_key)
        .ttl(3600)
        .mint(user_key)
    )

    agent = TenuoAgentQLAgent(warrant=warrant)

    # Benchmark allowed action
    print("\n1a. Testing ALLOWED action (navigate to allowed URL)...")
    bench = Benchmark()
    results_allowed = bench.measure_latency(
        lambda: agent.authorize("navigate", {"url": "https://example.com/page"}),
        iterations=1000
    )

    print(f"\n  Iterations: {results_allowed['iterations']}")
    print(f"  Mean:       {results_allowed['mean']:.3f} ms")
    print(f"  Median:     {results_allowed['median']:.3f} ms")
    print(f"  Min:        {results_allowed['min']:.3f} ms")
    print(f"  Max:        {results_allowed['max']:.3f} ms")
    print(f"  P95:        {results_allowed['p95']:.3f} ms")
    print(f"  P99:        {results_allowed['p99']:.3f} ms")

    # Benchmark denied action
    print("\n1b. Testing DENIED action (navigate to disallowed URL)...")

    def check_denied():
        try:
            agent.authorize("navigate", {"url": "https://malicious.com"})
        except Exception:
            pass  # Expected to fail

    results_denied = bench.measure_latency(check_denied, iterations=1000)

    print(f"\n  Iterations: {results_denied['iterations']}")
    print(f"  Mean:       {results_denied['mean']:.3f} ms")
    print(f"  Median:     {results_denied['median']:.3f} ms")
    print(f"  Min:        {results_denied['min']:.3f} ms")
    print(f"  Max:        {results_denied['max']:.3f} ms")
    print(f"  P95:        {results_denied['p95']:.3f} ms")
    print(f"  P99:        {results_denied['p99']:.3f} ms")

    # Verify claims
    print("\n" + "-"*70)
    print("VERIFICATION:")
    print("-"*70)

    claim_latency = 0.005  # Our claim in PERFORMANCE.md (actual ~0.004ms + 20% buffer)
    actual_allowed = results_allowed['mean']
    actual_denied = results_denied['mean']

    if actual_allowed <= claim_latency * 2:  # Within 2x of claim
        print(f"‚úÖ PASS: Allowed action latency ({actual_allowed:.3f}ms) matches claim (~{claim_latency}ms)")
    else:
        print(f"‚ö†Ô∏è  WARN: Allowed action latency ({actual_allowed:.3f}ms) exceeds 2x claim ({claim_latency*2}ms)")

    if actual_denied <= claim_latency * 2:
        print(f"‚úÖ PASS: Denied action latency ({actual_denied:.3f}ms) matches claim (~{claim_latency}ms)")
    else:
        print(f"‚ö†Ô∏è  WARN: Denied action latency ({actual_denied:.3f}ms) exceeds 2x claim ({claim_latency*2}ms)")

    return results_allowed, results_denied


def benchmark_throughput():
    """Benchmark 2: Throughput (checks per second)."""
    print("\n" + "="*70)
    print("BENCHMARK 2: Throughput (Authorizations per Second)")
    print("="*70)
    print("\nMeasuring how many authorization checks can be performed per second...")

    # Setup
    user_key = SigningKey.generate()
    agent_key = SigningKey.generate()

    warrant = (Warrant.mint_builder()
        .capability("navigate", url=UrlPattern("https://*.example.com/*"))
        .holder(agent_key.public_key)
        .ttl(3600)
        .mint(user_key)
    )

    agent = TenuoAgentQLAgent(warrant=warrant)

    bench = Benchmark()

    # Test throughput
    print("\nRunning for 2 seconds...")
    throughput = bench.measure_throughput(
        lambda: agent.authorize("navigate", {"url": "https://example.com"}),
        duration_sec=2.0
    )

    print(f"\n  Throughput: {throughput:,.0f} checks/second")

    # Verify claims
    print("\n" + "-"*70)
    print("VERIFICATION:")
    print("-"*70)

    claim_throughput = 10000  # Our claim in PERFORMANCE.md

    if throughput >= claim_throughput * 0.5:  # Within 50% of claim
        print(f"‚úÖ PASS: Throughput ({throughput:,.0f} checks/sec) matches claim (~{claim_throughput:,} checks/sec)")
    else:
        print(f"‚ö†Ô∏è  WARN: Throughput ({throughput:,.0f} checks/sec) below 50% of claim ({claim_throughput:,} checks/sec)")

    print(f"\n  With {throughput:,.0f} checks/sec:")
    print(f"  - Can handle {throughput*60:,.0f} actions/minute")
    print(f"  - Can handle {throughput*3600:,.0f} actions/hour")
    print(f"  - For typical agent (100 actions/hour): {throughput*36:,.0f} concurrent agents")

    return throughput


def benchmark_delegation_overhead():
    """Benchmark 3: Delegation chain verification overhead."""
    print("\n" + "="*70)
    print("BENCHMARK 3: Delegation Chain Overhead")
    print("="*70)
    print("\nMeasuring overhead of verifying delegation chains...")

    # Setup
    root_key = SigningKey.generate()
    orchestrator_key = SigningKey.generate()
    worker_key = SigningKey.generate()

    # Create a 2-level delegation chain
    root_warrant = (Warrant.mint_builder()
        .capability("navigate", url=UrlPattern("https://*.example.com/*"))
        .holder(orchestrator_key.public_key)
        .ttl(3600)
        .mint(root_key)
    )

    delegated_warrant = (root_warrant.grant_builder()
        .capability("navigate", url=UrlPattern("https://sub.example.com/*"))
        .holder(worker_key.public_key)
        .ttl(1800)
        .grant(orchestrator_key)
    )

    # Benchmark root warrant (no delegation)
    print("\n3a. Root warrant (depth=0)...")
    root_agent = TenuoAgentQLAgent(warrant=root_warrant)

    bench = Benchmark()
    results_root = bench.measure_latency(
        lambda: root_agent.authorize("navigate", {"url": "https://example.com"}),
        iterations=1000
    )

    print(f"  Mean latency: {results_root['mean']:.3f} ms")

    # Benchmark delegated warrant (depth=1)
    print("\n3b. Delegated warrant (depth=1)...")
    worker_agent = TenuoAgentQLAgent(warrant=delegated_warrant)

    results_delegated = bench.measure_latency(
        lambda: worker_agent.authorize("navigate", {"url": "https://sub.example.com"}),
        iterations=1000
    )

    print(f"  Mean latency: {results_delegated['mean']:.3f} ms")

    # Calculate overhead
    overhead = results_delegated['mean'] - results_root['mean']
    overhead_pct = (overhead / results_root['mean']) * 100

    print("\n" + "-"*70)
    print("VERIFICATION:")
    print("-"*70)
    print(f"\n  Root warrant (depth=0):      {results_root['mean']:.3f} ms")
    print(f"  Delegated warrant (depth=1): {results_delegated['mean']:.3f} ms")
    print(f"  Overhead per delegation:     {overhead:.3f} ms ({overhead_pct:.1f}%)")

    # Reasonable overhead should be < 2x
    if results_delegated['mean'] < results_root['mean'] * 2:
        print("\n‚úÖ PASS: Delegation overhead is reasonable (<2x root)")
    else:
        print("\n‚ö†Ô∏è  WARN: Delegation overhead is high (>2x root)")

    return results_root, results_delegated


def benchmark_memory_usage():
    """Benchmark 4: Memory overhead."""
    print("\n" + "="*70)
    print("BENCHMARK 4: Memory Overhead")
    print("="*70)

    try:
        import psutil
        import os

        process = psutil.Process(os.getpid())

        # Measure baseline
        baseline_mb = process.memory_info().rss / 1024 / 1024
        print(f"\nBaseline memory: {baseline_mb:.2f} MB")

        # Create 100 agents
        print("\nCreating 100 agents with warrants...")

        agents = []
        for i in range(100):
            user_key = SigningKey.generate()
            agent_key = SigningKey.generate()

            warrant = (Warrant.mint_builder()
                .capability("navigate", url=UrlPattern("https://*.example.com/*"))
                .capability("fill", element=OneOf(["search_box"]))
                .holder(agent_key.public_key)
                .ttl(3600)
                .mint(user_key)
            )

            agent = TenuoAgentQLAgent(warrant=warrant)
            agents.append(agent)

        # Measure after creating agents
        after_mb = process.memory_info().rss / 1024 / 1024
        overhead_mb = after_mb - baseline_mb
        per_agent_kb = (overhead_mb * 1024) / 100

        print(f"After 100 agents: {after_mb:.2f} MB")
        print(f"Overhead: {overhead_mb:.2f} MB")
        print(f"Per agent: {per_agent_kb:.1f} KB")

        print("\n" + "-"*70)
        print("VERIFICATION:")
        print("-"*70)

        claim_per_agent = 50  # KB, our claim in PERFORMANCE.md

        if per_agent_kb <= claim_per_agent * 2:
            print(f"‚úÖ PASS: Memory per agent ({per_agent_kb:.1f} KB) matches claim (~{claim_per_agent} KB)")
        else:
            print(f"‚ö†Ô∏è  WARN: Memory per agent ({per_agent_kb:.1f} KB) exceeds 2x claim ({claim_per_agent*2} KB)")

        return per_agent_kb

    except ImportError:
        print("\n‚ö†Ô∏è  SKIP: psutil not installed. Install with: pip install psutil")
        print("   Memory benchmarking requires psutil to measure memory usage.")
        return None


def benchmark_workflow_overhead():
    """Benchmark 5: Real workflow overhead comparison."""
    print("\n" + "="*70)
    print("BENCHMARK 5: Workflow Overhead")
    print("="*70)
    print("\nSimulating typical browser automation workflow...")
    print("(Measuring authorization overhead as % of total time)")

    # Setup
    user_key = SigningKey.generate()
    agent_key = SigningKey.generate()

    warrant = (Warrant.mint_builder()
        .capability("navigate", url=UrlPattern("https://*.example.com/*"))
        .capability("fill", element=Wildcard())
        .capability("click", element=Wildcard())
        .holder(agent_key.public_key)
        .ttl(3600)
        .mint(user_key)
    )

    agent = TenuoAgentQLAgent(warrant=warrant)

    # Simulate workflow: 10 actions with realistic delays
    actions = [
        ("navigate", {"url": "https://example.com"}, 500),  # 500ms page load
        ("fill", {"element": "search_box"}, 50),             # 50ms fill
        ("click", {"element": "search_button"}, 100),        # 100ms click
        ("navigate", {"url": "https://example.com/results"}, 500),
        ("fill", {"element": "filter"}, 50),
        ("click", {"element": "item"}, 100),
        ("navigate", {"url": "https://example.com/item"}, 500),
        ("click", {"element": "buy_button"}, 100),
        ("fill", {"element": "quantity"}, 50),
        ("click", {"element": "checkout"}, 100),
    ]

    print(f"\nSimulating {len(actions)} actions with realistic browser delays...")
    print("(navigate: 500ms, click: 100ms, fill: 50ms)")

    total_action_time = 0
    total_auth_time = 0

    for action_type, args, simulated_delay_ms in actions:
        # Measure authorization
        auth_start = time.perf_counter()
        agent.authorize(action_type, args)
        auth_end = time.perf_counter()
        auth_time = (auth_end - auth_start) * 1000

        # Simulate browser action
        time.sleep(simulated_delay_ms / 1000)

        total_auth_time += auth_time
        total_action_time += simulated_delay_ms

    total_time = total_action_time + total_auth_time
    overhead_pct = (total_auth_time / total_time) * 100

    print(f"\n  Total workflow time:    {total_time:.1f} ms")
    print(f"  Browser action time:    {total_action_time:.1f} ms ({total_action_time/total_time*100:.1f}%)")
    print(f"  Authorization time:     {total_auth_time:.3f} ms ({overhead_pct:.2f}%)")
    print(f"  Average auth per check: {total_auth_time/len(actions):.3f} ms")

    print("\n" + "-"*70)
    print("VERIFICATION:")
    print("-"*70)

    claim_overhead_pct = 1.0  # Our claim: <1% overhead

    if overhead_pct < claim_overhead_pct:
        print(f"‚úÖ PASS: Workflow overhead ({overhead_pct:.2f}%) is less than claimed ({claim_overhead_pct}%)")
    else:
        print(f"‚ö†Ô∏è  INFO: Workflow overhead ({overhead_pct:.2f}%) exceeds claim ({claim_overhead_pct}%)")
        print("         This is expected in fast simulations. Real browser actions are slower.")

    return overhead_pct


def print_summary(results):
    """Print summary of all benchmarks."""
    print("\n" + "="*70)
    print("BENCHMARK SUMMARY")
    print("="*70)

    print("\nüìä Performance Claims vs Actual:")
    print("-"*70)

    # Extract results
    latency = results.get('latency_allowed', {}).get('mean', 0)
    throughput = results.get('throughput', 0)
    memory = results.get('memory_per_agent', 0)
    overhead = results.get('workflow_overhead', 0)

    print("\n1. Authorization Latency:")
    print("   Claim: ~0.005ms")
    print(f"   Actual: {latency:.3f}ms")
    print(f"   Status: {'‚úÖ PASS' if latency <= 0.01 else '‚ö†Ô∏è  REVIEW'}")

    print("\n2. Throughput:")
    print("   Claim: ~268,000 checks/second")
    print(f"   Actual: {throughput:,.0f} checks/second")
    print(f"   Status: {'‚úÖ PASS' if throughput >= 100000 else '‚ö†Ô∏è  REVIEW'}")

    if memory:
        print("\n3. Memory per Agent:")
        print("   Claim: ~50 KB")
        print(f"   Actual: {memory:.1f} KB")
        print(f"   Status: {'‚úÖ PASS' if memory <= 100 else '‚ö†Ô∏è  REVIEW'}")

    print("\n4. Workflow Overhead:")
    print("   Claim: <0.03%")
    print(f"   Actual: {overhead:.2f}%")
    print("   Note: In real workflows with slower browser actions, overhead approaches 0%")

    print("\n" + "="*70)
    print("\nüí° To update PERFORMANCE.md with these results:")
    print("   1. Review the numbers above")
    print("   2. Update PERFORMANCE.md with actual measured values")
    print("   3. Include system specifications (shown above) for reproducibility")
    print("   4. Run this benchmark periodically to verify claims")
    print()


def print_system_info():
    """Print system information for reproducibility."""
    print("\n" + "="*70)
    print("SYSTEM INFORMATION")
    print("="*70)
    print(f"\nPlatform: {platform.system()} {platform.release()}")
    print(f"Machine: {platform.machine()}")
    print(f"Processor: {platform.processor()}")
    print(f"Python: {platform.python_version()}")

    # Try to get more detailed info on macOS
    if platform.system() == "Darwin":
        try:
            import subprocess
            result = subprocess.run(
                ["sysctl", "-n", "machdep.cpu.brand_string"],
                capture_output=True,
                text=True,
                timeout=1
            )
            if result.returncode == 0:
                print(f"CPU: {result.stdout.strip()}")
        except Exception:
            pass

    print("\n" + "-"*70)


def main():
    """Run all benchmarks."""
    print("="*70)
    print("Tenuo Performance Benchmark Suite")
    print("="*70)
    print("\nThis benchmark verifies the performance claims in PERFORMANCE.md")
    print("Run time: ~10 seconds")

    print_system_info()

    results = {}

    try:
        # Benchmark 1: Latency
        allowed, denied = benchmark_authorization_latency()
        results['latency_allowed'] = allowed
        results['latency_denied'] = denied

        # Benchmark 2: Throughput
        throughput = benchmark_throughput()
        results['throughput'] = throughput

        # Benchmark 3: Delegation
        root, delegated = benchmark_delegation_overhead()
        results['delegation_root'] = root
        results['delegation_child'] = delegated

        # Benchmark 4: Memory
        memory = benchmark_memory_usage()
        results['memory_per_agent'] = memory

        # Benchmark 5: Workflow
        overhead = benchmark_workflow_overhead()
        results['workflow_overhead'] = overhead

        # Summary
        print_summary(results)

        return 0

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Benchmark interrupted by user")
        return 1
    except Exception as e:
        print(f"\n\n‚ùå Benchmark failed: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
